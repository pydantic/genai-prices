# yaml-language-server: $schema=.schema.json
name: Groq
id: groq
pricing_urls:
  - https://groq.com/pricing/
api_pattern: 'https://api\.groq\.com'

# https://github.com/groq/groq-python/blob/v0.31.0/src/groq/types/completion_usage.py
extractors:
  - root: usage
    mappings:
      - path: prompt_tokens
        dest: input_tokens
      - path: completion_tokens
        dest: output_tokens

models:
  - id: gemma-7b-it
    match:
      equals: gemma-7b-it
    prices:
      input_mtok: 0.07
      output_mtok: 0.07

  - id: gemma2-9b-it
    match:
      equals: gemma2-9b-it
    prices:
      input_mtok: 0.2
      output_mtok: 0.2

  - id: llama2-70b-4096
    match:
      equals: llama2-70b-4096
    prices:
      input_mtok: 0.7
      output_mtok: 0.8

  - id: llama3-70b-8192
    match:
      equals: llama3-70b-8192
    prices:
      input_mtok: 0.59
      output_mtok: 0.79

  - id: llama3-8b-8192
    match:
      equals: llama3-8b-8192
    prices:
      input_mtok: 0.05
      output_mtok: 0.08

  - id: llama3-groq-70b-8192-tool-use-preview
    match:
      equals: llama3-groq-70b-8192-tool-use-preview
    prices:
      input_mtok: 0.89
      output_mtok: 0.89

  - id: llama3-groq-8b-8192-tool-use-preview
    match:
      equals: llama3-groq-8b-8192-tool-use-preview
    prices:
      input_mtok: 0.19
      output_mtok: 0.19

  - id: mixtral-8x7b-32768
    match:
      equals: mixtral-8x7b-32768
    prices:
      input_mtok: 0.24
      output_mtok: 0.24

  # https://console.groq.com/docs/model/openai/gpt-oss-120b
  - id: openai/gpt-oss-120b
    description: |
      GPT-OSS 120B is OpenAI's flagship open source model, built on a Mixture-of-Experts (MoE) architecture with
      120 billion parameters and 128 experts.
    match:
      equals: openai/gpt-oss-120b
    context_window: 131072
    prices_checked: 2025-08-06
    prices:
      input_mtok: 0.15
      output_mtok: 0.75

  # https://console.groq.com/docs/model/openai/gpt-oss-20b
  - id: openai/gpt-oss-20b
    description: |
      GPT-OSS 20B is OpenAI's flagship open source model, built on a Mixture-of-Experts (MoE) architecture with
      20 billion parameters and 32 experts.
    match:
      equals: openai/gpt-oss-20b
    context_window: 131072
    prices_checked: 2025-08-06
    prices:
      input_mtok: 0.10
      output_mtok: 0.50
