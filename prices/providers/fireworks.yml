# yaml-language-server: $schema=.schema.json
name: Fireworks
id: fireworks
pricing_urls:
  - https://fireworks.ai/pricing
api_pattern: 'https://api\.fireworks\.ai'
model_match:
  starts_with: accounts/fireworks/models/

extractors:
  # assuming this API matches OpenAI
  - api_flavor: chat
    root: usage
    mappings:
      - path: prompt_tokens
        dest: input_tokens
      - path: [prompt_tokens_details, cached_tokens]
        dest: cache_read_tokens
        required: false
      - path: [completion_tokens_details, audio_tokens]
        dest: output_audio_tokens
        required: false
      - path: completion_tokens
        dest: output_tokens

models:
  - id: deepseek-r1-0528
    name: DeepSeek R1 0528
    description: >-
      The updated DeepSeek-R1-0528 model delivers major improvements in reasoning, inference, and accuracy through enhanced post-training optimization and greater computational resources. It now performs at a level approaching top-tier models like O3 and Gemini 2.5 Pro, with notable gains in complex tasks such as math and programming.
    match:
      equals: accounts/fireworks/models/deepseek-r1-0528
    context_window: 160000
    prices_checked: 2025-07-13
    prices:
      input_mtok: 3
      output_mtok: 8

  - id: deepseek-v3-0324
    name: Deepseek V3 03-24
    description: >-
      A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token from Deepseek. Updated checkpoint.
    match:
      equals: accounts/fireworks/models/deepseek-v3-0324
    context_window: 160000
    prices_checked: 2025-07-13
    price_comments: docs give just one price - "Pricing Per 1M Tokens", we assume that's input and output
    prices:
      input_mtok: 0.9
      output_mtok: 0.9

  - id: deepseek-v3p2
    name: Deepseek V3.2
    description: >-
      Model from Deepseek that harmonizes high computational efficiency with superior reasoning and agent performance. 675B parameter MoE model.
    match:
      equals: accounts/fireworks/models/deepseek-v3p2
    context_window: 163840
    prices_checked: 2026-02-03
    prices:
      input_mtok: 0.56
      cache_read_mtok: 0.28
      output_mtok: 1.68

  - id: gemma-3-27b-it
    name: Gemma 3 27B Instruct
    match:
      equals: accounts/fireworks/models/gemma-3-27b-it
    context_window: 131000
    prices_checked: 2025-07-13
    price_comments: docs give just one price - "Pricing Per 1M Tokens", we assume that's input and output
    prices:
      input_mtok: 0.1
      output_mtok: 0.1

  - id: glm-4p7
    name: GLM-4.7
    description: >-
      Next-generation general-purpose model from Z.ai optimized for coding, reasoning, and agentic workflows. 352B parameter MoE model with advanced thinking controls.
    match:
      equals: accounts/fireworks/models/glm-4p7
    context_window: 202752
    prices_checked: 2026-02-03
    prices:
      input_mtok: 0.60
      output_mtok: 2.20

  - id: gpt-oss-120b
    name: OpenAI gpt-oss-120b
    description: >-
      OpenAI's open-weight 117B parameter MoE model designed for production, general purpose, high reasoning use-cases. Features powerful reasoning, agentic tasks, and versatile developer use cases.
    match:
      equals: accounts/fireworks/models/gpt-oss-120b
    context_window: 131072
    prices_checked: 2026-02-03
    prices:
      input_mtok: 0.15
      cache_read_mtok: 0.07
      output_mtok: 0.60

  - id: gpt-oss-20b
    name: OpenAI gpt-oss-20b
    description: >-
      OpenAI's open-weight 21.5B parameter model designed for powerful reasoning, agentic tasks, and versatile developer use cases. Optimized for lower latency and local or specialized tasks.
    match:
      equals: accounts/fireworks/models/gpt-oss-20b
    context_window: 131072
    prices_checked: 2026-02-03
    prices:
      input_mtok: 0.07
      cache_read_mtok: 0.04
      output_mtok: 0.30

  - id: kimi-k2p5
    name: Kimi K2.5
    description: >-
      Moonshot AI's flagship agentic model. Unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. 1T parameter MoE model.
    match:
      equals: accounts/fireworks/models/kimi-k2p5
    context_window: 262144
    prices_checked: 2026-02-03
    prices:
      input_mtok: 0.60
      cache_read_mtok: 0.10
      output_mtok: 3.00

  - id: llama-v3p1-8b-instruct
    name: Llama 3.1 8B Instruct
    description: >-
      The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes. The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.
    match:
      equals: accounts/fireworks/models/llama-v3p1-8b-instruct
    context_window: 131000
    prices_checked: 2025-07-13
    price_comments: docs give just one price - "Pricing Per 1M Tokens", we assume that's input and output
    prices:
      input_mtok: 0.2
      output_mtok: 0.2

  - id: llama4-maverick-instruct-basic
    name: Llama 4 Maverick Instruct (Basic)
    description: >-
      The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes. The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.
    match:
      equals: accounts/fireworks/models/llama4-maverick-instruct-basic
    context_window: 1000000
    prices_checked: 2025-07-13
    prices:
      input_mtok: 0.22
      output_mtok: 0.88

  - id: minimax-m2p1
    name: MiniMax-M2.1
    description: >-
      Built for strong real-world performance across complex, multi-language, and agent-driven workflows. 228B parameter model with robust support for systems, backend, web, mobile, and office-style tasks.
    match:
      equals: accounts/fireworks/models/minimax-m2p1
    context_window: 204800
    prices_checked: 2026-02-03
    prices:
      input_mtok: 0.30
      output_mtok: 1.20

  - id: qwen2p5-vl-72b-instruct
    name: Qwen2.5-VL 72B Instruct
    description: Latest Qwen's VLM model
    match:
      equals: accounts/fireworks/models/qwen2p5-vl-72b-instruct
    context_window: 128000
    prices_checked: 2025-07-13
    price_comments: docs give just one price - "Pricing Per 1M Tokens", we assume that's input and output
    prices:
      input_mtok: 0.9
      output_mtok: 0.9

  - id: qwen3-235b-a22b
    name: Qwen3 235B-A22B
    description: >-
      Qwen3 is the latest evolution in the Qwen LLM series, featuring both dense and MoE models with major advancements in reasoning, agent capabilities, multilingual support, and instruction following. It uniquely allows seamless switching between "thinking" (for complex logic, math, coding) and "non-thinking" modes (for fast, general dialogue), delivering strong performance across tasks.
    match:
      equals: accounts/fireworks/models/qwen3-235b-a22b
    context_window: 128000
    prices_checked: 2025-07-13
    prices:
      input_mtok: 0.22
      output_mtok: 0.88
