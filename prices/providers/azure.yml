# yaml-language-server: $schema=.schema.json
name: Microsoft Azure
id: azure
pricing_urls:
  - https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/#pricing
api_pattern: (https?://)?([^.]*\.)?(?:openai\.azure\.com|azure-api\.net|cognitiveservices\.azure\.com)
price_comments: >-
  These are prices for "*-Global" models, prices for "Regional" models are often slightly hihgher.
models:
  - id: ada
    match:
      equals: ada
    prices:
      input_mtok: 0.4
      output_mtok: 0.4

  - id: babbage
    match:
      equals: babbage
    prices:
      input_mtok: 0.5
      output_mtok: 0.5

  - id: curie
    match:
      equals: curie
    prices:
      input_mtok: 2
      output_mtok: 2

  - id: davinci
    match:
      equals: davinci
    prices:
      input_mtok: 20
      output_mtok: 20

  - id: gpt-3.5-turbo
    match:
      or:
        - equals: gpt-3.5-turbo
        - equals: gpt-3.5-turbo-0301
        - equals: gpt-3.5-turbo-0613
        - equals: gpt-3.5-turbo-instruct
        - equals: gpt-3.5-turbo-instruct-0914
    prices:
      input_mtok: 1.5
      output_mtok: 2

  - id: gpt-3.5-turbo-0125
    match:
      equals: gpt-3.5-turbo-0125
    prices:
      input_mtok: 0.5
      output_mtok: 1.5

  - id: gpt-3.5-turbo-1106
    match:
      equals: gpt-3.5-turbo-1106
    prices:
      input_mtok: 1
      output_mtok: 2

  - id: gpt-3.5-turbo-16k-0613
    match:
      equals: gpt-3.5-turbo-16k-0613
    prices:
      input_mtok: 3
      output_mtok: 4

  - id: gpt-35-16k
    match:
      equals: gpt-35-16k
    prices:
      input_mtok: 3
      output_mtok: 4

  - id: gpt-35-turbo
    match:
      or:
        - equals: gpt-35-turbo
        - equals: gpt-35-turbo-0613
        - equals: gpt-35-turbo-1106
    prices:
      input_mtok: 1.5
      output_mtok: 2

  - id: gpt-35-turbo-16k
    match:
      or:
        - equals: gpt-35-turbo-16k
        - equals: gpt-35-turbo-16k-0613
    prices:
      input_mtok: 3
      output_mtok: 4

  - id: gpt-4
    match:
      or:
        - equals: gpt-4
        - equals: gpt-4-0314
        - equals: gpt-4-0613
    prices:
      input_mtok: 30
      output_mtok: 60

  - id: gpt-4-0125-preview
    match:
      equals: gpt-4-0125-preview
    prices:
      input_mtok: 10
      output_mtok: 30

  - id: gpt-4-1106-preview
    match:
      equals: gpt-4-1106-preview
    prices:
      input_mtok: 10
      output_mtok: 30

  - id: gpt-4-1106-vision-preview
    match:
      equals: gpt-4-1106-vision-preview
    prices:
      input_mtok: 10
      output_mtok: 30

  - id: gpt-4-32k
    match:
      or:
        - equals: gpt-4-32k
        - equals: gpt-4-32k-0314
        - equals: gpt-4-32k-0613
    prices:
      input_mtok: 60
      output_mtok: 120

  - id: gpt-4-preview-1106
    match:
      equals: gpt-4-preview-1106
    prices:
      input_mtok: 10
      output_mtok: 30

  - id: gpt-4-turbo
    match:
      or:
        - equals: gpt-4-turbo
        - equals: gpt-4-turbo-0125-preview
        - equals: gpt-4-turbo-2024-04-09
        - equals: gpt-4-turbo-preview
    prices:
      input_mtok: 10
      output_mtok: 30

  - id: gpt-4-vision
    match:
      or:
        - equals: gpt-4-vision
        - equals: gpt-4-vision-preview
    prices:
      input_mtok: 10
      output_mtok: 30

  - id: gpt-4.1
    match:
      or:
        - equals: gpt-4.1
        - equals: gpt-4.1-2025-04-14
    prices_checked: 2025-07-03
    prices:
      input_mtok: 2
      cache_read_mtok: 0.5
      output_mtok: 8

  - id: gpt-4.1-mini
    match:
      or:
        - equals: gpt-4.1-mini
        - equals: gpt-4.1-mini-2025-04-14
    prices_checked: 2025-07-03
    prices:
      input_mtok: 0.4
      cache_read_mtok: 0.1
      output_mtok: 1.6

  - id: gpt-4.1-nano
    match:
      or:
        - equals: gpt-4.1-nano
        - equals: gpt-4.1-nano-2025-04-14
    prices_checked: 2025-07-03
    prices:
      input_mtok: 0.1
      cache_read_mtok: 0.03
      output_mtok: 0.4

  - id: gpt-45-turbo
    match:
      equals: gpt-45-turbo
    prices:
      input_mtok: 10
      output_mtok: 30

  - id: gpt-4o-2024-05-13
    match:
      or:
        - equals: gpt-4o-2024-05-13
        - equals: gpt-4o-2024-0513
    prices_checked: 2025-07-03
    prices:
      input_mtok: 5
      output_mtok: 15

  - id: gpt-4o-2024-08-06
    match:
      equals: gpt-4o-2024-08-06
    prices_checked: 2025-07-03
    prices:
      input_mtok: 2.5
      cache_read_mtok: 1.25
      output_mtok: 10

  - id: gpt-4o-2024-11-20
    match:
      or:
        - equals: gpt-4o-2024-11-20
        - equals: gpt-4o-2024-1120
    prices_checked: 2025-07-03
    prices:
      input_mtok: 2.5
      output_mtok: 10

  - id: gpt-4o-mini
    context_window: 128000
    match:
      starts_with: gpt-4o-mini
    prices_checked: 2025-07-03
    prices:
      input_mtok: 0.15
      cache_read_mtok: 0.075
      output_mtok: 0.6

  - id: gpt-4o-mini-realtime
    match:
      starts_with: gpt-4o-mini-realtime
    prices_checked: 2025-07-03
    prices:
      input_mtok: 0.6
      output_mtok: 2.4
      input_audio_mtok: 10
      cache_audio_read_mtok: 0.3
      output_audio_mtok: 20

  - id: gpt-4o-realtime
    match:
      starts_with: gpt-4o-realtime
    prices_checked: 2025-07-03
    prices:
      input_mtok: 5
      cache_read_mtok: 2.5
      output_mtok: 20
      input_audio_mtok: 40
      cache_audio_read_mtok: 2.5
      output_audio_mtok: 80

  - id: gpt35
    match:
      equals: gpt35
    prices:
      input_mtok: 1.5
      output_mtok: 2

  - id: gpt4-turbo-preview
    match:
      equals: gpt4-turbo-preview
    prices:
      input_mtok: 10
      output_mtok: 30

  - id: mai-ds-r1:free
    name: MAI DS R1 (free)
    description: >-
      MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed by the Microsoft AI team to improve the model's responsiveness
      on previously blocked topics while enhancing its safety profile. Built on
      top of DeepSeek-R1's reasoning foundation, it integrates 110k examples from the Tulu-3 SFT dataset and 350k internally
      curated multilingual safety-alignment samples. The model retains strong reasoning,
      coding, and problem-solving capabilities, while unblocking a wide range of prompts previously restricted in R1.
    match:
      equals: mai-ds-r1:free
    prices: {}

  - id: o1
    match:
      or:
        - equals: o1
        - equals: o1-2024-12-17
        - equals: o1-preview
        - equals: o1-preview-2024-09-12
    prices_checked: 2025-07-03
    prices:
      input_mtok: 15
      cache_read_mtok: 7.5
      output_mtok: 60

  - id: o1-mini
    match:
      or:
        - equals: o1-mini
        - equals: o1-mini-2024-09-12
    prices_checked: 2025-07-03
    prices:
      input_mtok: 1.1
      cache_read_mtok: 0.55
      output_mtok: 4.4

  - id: o3-2025-04-16
    match:
      or:
        - equals: o3
        - equals: o3-2025-04-16
    prices_checked: 2025-07-05
    prices:
      input_mtok: 2
      cache_read_mtok: 0.5
      output_mtok: 8

  - id: o3-mini
    match:
      or:
        - equals: o3-mini
        - equals: o3-mini-2025-01-31
    prices_checked: 2025-07-03
    prices:
      input_mtok: 1.1
      cache_read_mtok: 0.55
      output_mtok: 4.4

  - id: o4-mini
    match:
      or:
        - contains: o4-mini
        - contains: o4-mini-2025-04-16
    prices_checked: 2025-07-05
    prices:
      input_mtok: 1.1
      cache_read_mtok: 0.28
      output_mtok: 4.4

  - id: phi-3-medium-128k-instruct
    name: Phi-3 Medium 128K Instruct
    description: >-
      Phi-3 128K Medium is a powerful 14-billion parameter model designed for advanced language understanding, reasoning,
      and instruction following. Optimized through supervised fine-tuning and preference
      adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing.
    match:
      equals: phi-3-medium-128k-instruct
    prices:
      input_mtok: 1
      output_mtok: 1

  - id: phi-3-mini-128k-instruct
    name: Phi-3 Mini 128K Instruct
    description: >-
      Phi-3 Mini is a powerful 3.8B parameter model designed for advanced language understanding, reasoning, and instruction
      following. Optimized through supervised fine-tuning and preference adjustments,
      it excels in tasks involving common sense, mathematics, logical reasoning, and code processing.
    match:
      equals: phi-3-mini-128k-instruct
    prices:
      input_mtok: 0.1
      output_mtok: 0.1

  - id: phi-3.5-mini-128k-instruct
    name: Phi-3.5 Mini 128K Instruct
    description: >-
      Phi-3.5 models are lightweight, state-of-the-art open models. These models were trained with Phi-3 datasets that include
      both synthetic data and the filtered, publicly available websites data, with
      a focus on high quality and reasoning-dense properties. Phi-3.5 Mini uses 3.8B parameters, and is a dense decoder-only
      transformer model using the same tokenizer as Phi-3 Mini.
    match:
      equals: phi-3.5-mini-128k-instruct
    prices:
      input_mtok: 0.1
      output_mtok: 0.1

  - id: phi-4
    name: Phi 4
    description: >-
      Microsoft Research Phi-4 is designed to perform well in complex reasoning tasks and can operate efficiently in situations
      with limited memory or where quick responses are needed.
    match:
      equals: phi-4
    prices:
      input_mtok: 0.07
      output_mtok: 0.14

  - id: phi-4-multimodal-instruct
    name: Phi 4 Multimodal Instruct
    description: >-
      Phi-4 Multimodal Instruct is a versatile 5.6B parameter foundation model that combines advanced reasoning and instruction-following
      capabilities across both text and visual inputs, providing accurate
      text outputs. The unified architecture enables efficient, low-latency inference, suitable for edge and mobile deployments.
      Phi-4 Multimodal Instruct supports text inputs in multiple languages including
      Arabic, Chinese, English, French, German, Japanese, Spanish, and more, with visual input optimized primarily for English.
      It delivers impressive performance on multimodal tasks involving mathematical,
      scientific, and document reasoning, providing developers and enterprises a powerful yet compact model for sophisticated
      interactive applications. For more information, see the Phi-4 Multimodal blog
      post.
    match:
      equals: phi-4-multimodal-instruct
    prices:
      input_mtok: 0.05
      output_mtok: 0.1

  - id: phi-4-reasoning-plus
    name: Phi 4 Reasoning Plus
    description: >-
      Phi-4-reasoning-plus is an enhanced 14B parameter model from Microsoft, fine-tuned from Phi-4 with additional reinforcement
      learning to boost accuracy on math, science, and code reasoning tasks. It
      uses the same dense decoder-only transformer architecture as Phi-4, but generates longer, more comprehensive outputs
      structured into a step-by-step reasoning trace and final answer.
    match:
      equals: phi-4-reasoning-plus
    prices:
      input_mtok: 0.07
      output_mtok: 0.35

  - id: phi-4-reasoning-plus:free
    name: Phi 4 Reasoning Plus (free)
    description: >-
      Phi-4-reasoning-plus is an enhanced 14B parameter model from Microsoft, fine-tuned from Phi-4 with additional reinforcement
      learning to boost accuracy on math, science, and code reasoning tasks. It
      uses the same dense decoder-only transformer architecture as Phi-4, but generates longer, more comprehensive outputs
      structured into a step-by-step reasoning trace and final answer.
    match:
      equals: phi-4-reasoning-plus:free
    prices: {}

  - id: phi-4-reasoning:free
    name: Phi 4 Reasoning (free)
    description: >-
      Phi-4-reasoning is a 14B parameter dense decoder-only transformer developed by Microsoft, fine-tuned from Phi-4 to enhance
      complex reasoning capabilities. It uses a combination of supervised fine-tuning
      on chain-of-thought traces and reinforcement learning, targeting math, science, and code reasoning tasks. With a 32k
      context window and high inference efficiency, it is optimized for structured responses
      in a two-part format: reasoning trace followed by a final solution.
    match:
      equals: phi-4-reasoning:free
    prices: {}

  - id: text-ada-001
    match:
      equals: text-ada-001
    prices:
      input_mtok: 0.4
      output_mtok: 0.4

  - id: text-curie-001
    match:
      equals: text-curie-001
    prices:
      input_mtok: 2
      output_mtok: 2

  - id: text-davinci-001
    match:
      equals: text-davinci-001
    prices:
      input_mtok: 20
      output_mtok: 20

  - id: text-davinci-002
    match:
      equals: text-davinci-002
    prices:
      input_mtok: 20
      output_mtok: 20

  - id: text-davinci-003
    match:
      equals: text-davinci-003
    prices:
      input_mtok: 20
      output_mtok: 20

  - id: text-embedding-3-large
    match:
      equals: text-embedding-3-large
    prices:
      input_mtok: 0.13

  - id: text-embedding-3-small
    match:
      equals: text-embedding-3-small
    prices:
      input_mtok: 0.02

  - id: text-embedding-ada
    match:
      or:
        - equals: text-embedding-ada
        - equals: text-embedding-ada-002
        - equals: text-embedding-ada-002-v2
    prices:
      input_mtok: 0.1

  - id: wizardlm-2-8x22b
    name: WizardLM-2 8x22B
    description: >-
      WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared
      to leading proprietary models, and it consistently outperforms all existing state-of-the-art
      opensource models.
    match:
      equals: wizardlm-2-8x22b
    prices:
      input_mtok: 0.48
      output_mtok: 0.48
